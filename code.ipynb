{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "import os\r\n",
    "import pandas as pd\r\n",
    "import collections\r\n",
    "findDictionary={\"No Finding\":0,\r\n",
    "                \"Atelectasis\":1,\r\n",
    "                \"Consolidation\":2,\r\n",
    "                \"Infiltration\":3,\r\n",
    "                \"Pneumothorax\":4,\r\n",
    "                \"Edema\":5,\r\n",
    "                \"Emphysema\":6,\r\n",
    "                \"Fibrosis\":7,\r\n",
    "                \"Effusion\":8,\r\n",
    "                \"Pneumonia\":9,\r\n",
    "                \"Pleural_Thickening\":10,\r\n",
    "                \"Cardiomegaly\":11,\r\n",
    "                \"Nodule\":12,\r\n",
    "                \"Hernia\":13,\r\n",
    "                \"Mass\":14}\r\n",
    "discoveriesWithFileNames = collections.defaultdict(list)\r\n",
    "findingsArr= collections.defaultdict(list)\r\n",
    "fileNameArr=[]\r\n",
    "discoverdDiseases=[]\r\n",
    "discoveredNames=[]\r\n",
    "data=pd.read_csv(\"Data_Entry_2017.csv\",usecols=[\"Image Index\",\"Finding Labels\"])\r\n",
    "for dirname, _, filenames in os.walk('./images'):\r\n",
    "    for filename in filenames:\r\n",
    "        related_index=data[data['Image Index']==filename].index.values\r\n",
    "        discoveries=data.loc[related_index,'Finding Labels'].values[0].split(\"|\")\r\n",
    "        discoveredNames.append(discoveries)\r\n",
    "        tem=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\r\n",
    "        for dis in discoveries:\r\n",
    "            tem[findDictionary[dis]]=1\r\n",
    "            discoveriesWithFileNames[dis].append(filename)\r\n",
    "        fileNameArr.append(filename)\r\n",
    "        discoverdDiseases.append(tem)\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "def column(matrix, i):\r\n",
    "    return [row[i] for row in matrix]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "arr1 = np.array([fileNameArr, discoveredNames,column(discoverdDiseases,0),column(discoverdDiseases,1),column(discoverdDiseases,2),column(discoverdDiseases,3),column(discoverdDiseases,4)\r\n",
    ",column(discoverdDiseases,5),column(discoverdDiseases,6),column(discoverdDiseases,7),column(discoverdDiseases,8),column(discoverdDiseases,9)\r\n",
    ",column(discoverdDiseases,10),column(discoverdDiseases,11),column(discoverdDiseases,12),column(discoverdDiseases,13),column(discoverdDiseases,14)])\r\n",
    "arr1_transpose = arr1.transpose()\r\n",
    "cities = pd.DataFrame(arr1_transpose, columns=['Id', 'Diseases',\"No Finding\",\"Atelectasis\",\"Consolidation\",\"Infiltration\",\"Pneumothorax\",\"Edema\",\"Emphysema\",\"Fibrosis\",\"Effusion\",\"Pneumonia\",\"Pleural_Thickening\",\"Cardiomegaly\",\"Nodule\",\"Hernia\",\"Mass\"])\r\n",
    "cities.to_csv('Findings.csv', index=False)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\anjan\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "import torch\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import torchvision.transforms as transforms\r\n",
    "from torch.utils.data import Dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "class ImageDataset(Dataset):\r\n",
    "    def __init__(self, csv, train, test):\r\n",
    "        self.csv = csv\r\n",
    "        self.train = train\r\n",
    "        self.test = test\r\n",
    "        self.all_image_names = self.csv[:]['Id']\r\n",
    "        self.all_labels = np.array(self.csv.drop(['Id', 'Diseases'], axis=1))\r\n",
    "        self.train_ratio = int(0.85 * len(self.csv))\r\n",
    "        self.valid_ratio = len(self.csv) - self.train_ratio\r\n",
    "        # set the training data images and labels\r\n",
    "        if self.train == True:\r\n",
    "            print(f\"Number of training images: {self.train_ratio}\")\r\n",
    "            self.image_names = list(self.all_image_names[:self.train_ratio])\r\n",
    "            self.labels = list(self.all_labels[:self.train_ratio])\r\n",
    "            # define the training transforms\r\n",
    "            self.transform = transforms.Compose([\r\n",
    "                transforms.ToPILImage(),\r\n",
    "                transforms.Resize((400, 400)),\r\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\r\n",
    "                transforms.RandomRotation(degrees=45),\r\n",
    "                transforms.ToTensor(),\r\n",
    "            ])\r\n",
    "        # set the validation data images and labels\r\n",
    "        elif self.train == False and self.test == False:\r\n",
    "            print(f\"Number of validation images: {self.valid_ratio}\")\r\n",
    "            self.image_names = list(self.all_image_names[-self.valid_ratio:-10])\r\n",
    "            self.labels = list(self.all_labels[-self.valid_ratio:])\r\n",
    "            # define the validation transforms\r\n",
    "            self.transform = transforms.Compose([\r\n",
    "                transforms.ToPILImage(),\r\n",
    "                transforms.Resize((400, 400)),\r\n",
    "                transforms.ToTensor(),\r\n",
    "            ])\r\n",
    "        # set the test data images and labels, only last 10 images\r\n",
    "        # this, we will use in a separate inference script\r\n",
    "        elif self.test == True and self.train == False:\r\n",
    "            self.image_names = list(self.all_image_names[-10:])\r\n",
    "            self.labels = list(self.all_labels[-10:])\r\n",
    "             # define the test transforms\r\n",
    "            self.transform = transforms.Compose([\r\n",
    "                transforms.ToPILImage(),\r\n",
    "                transforms.ToTensor(),\r\n",
    "            ])\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.image_names)\r\n",
    "    \r\n",
    "    def __getitem__(self, index):\r\n",
    "        image = cv2.imread(f\"images/{self.image_names[index]}\")\r\n",
    "        # convert the image from BGR to RGB color format\r\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n",
    "        # apply image transforms\r\n",
    "        image = self.transform(image)\r\n",
    "        targets = self.labels[index]\r\n",
    "        \r\n",
    "        return {\r\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\r\n",
    "            'label': torch.tensor(targets, dtype=torch.float32)\r\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "from torchvision import models as models\r\n",
    "import torch.nn as nn\r\n",
    "def model(pretrained, requires_grad):\r\n",
    "    model = models.resnet50(progress=True, pretrained=pretrained)\r\n",
    "    # to freeze the hidden layers\r\n",
    "    if requires_grad == False:\r\n",
    "        for param in model.parameters():\r\n",
    "            param.requires_grad = False\r\n",
    "    # to train the hidden layers\r\n",
    "    elif requires_grad == True:\r\n",
    "        for param in model.parameters():\r\n",
    "            param.requires_grad = True\r\n",
    "    # make the classification layer learnable\r\n",
    "    # we have 25 classes in total\r\n",
    "    model.fc = nn.Linear(2048, 15)\r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "import torch\r\n",
    "from tqdm import tqdm\r\n",
    "# training function\r\n",
    "def train(model, dataloader, optimizer, criterion, train_data, device):\r\n",
    "    print('Training')\r\n",
    "    model.train()\r\n",
    "    counter = 0\r\n",
    "    train_running_loss = 0.0\r\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\r\n",
    "        counter += 1\r\n",
    "        data, target = data['image'].to(device), data['label'].to(device)\r\n",
    "        optimizer.zero_grad()\r\n",
    "        outputs = model(data)\r\n",
    "        # apply sigmoid activation to get all the outputs between 0 and 1\r\n",
    "        outputs = torch.sigmoid(outputs)\r\n",
    "        loss = criterion(outputs, target)\r\n",
    "        train_running_loss += loss.item()\r\n",
    "        # backpropagation\r\n",
    "        loss.backward()\r\n",
    "        # update optimizer parameters\r\n",
    "        optimizer.step()\r\n",
    "        \r\n",
    "    train_loss = train_running_loss / counter\r\n",
    "    return train_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "# validation function\r\n",
    "def validate(model, dataloader, criterion, val_data, device):\r\n",
    "    print('Validating')\r\n",
    "    model.eval()\r\n",
    "    counter = 0\r\n",
    "    val_running_loss = 0.0\r\n",
    "    with torch.no_grad():\r\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\r\n",
    "            counter += 1\r\n",
    "            data, target = data['image'].to(device), data['label'].to(device)\r\n",
    "            outputs = model(data)\r\n",
    "            # apply sigmoid activation to get all the outputs between 0 and 1\r\n",
    "            outputs = torch.sigmoid(outputs)\r\n",
    "            loss = criterion(outputs, target)\r\n",
    "            val_running_loss += loss.item()\r\n",
    "        \r\n",
    "        val_loss = val_running_loss / counter\r\n",
    "        return val_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "matplotlib.style.use('ggplot')\r\n",
    "# initialize the computation device\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "source": [
    "#intialize the model\r\n",
    "model = model(pretrained=True, requires_grad=False).to(device)\r\n",
    "# learning parameters\r\n",
    "lr = 0.0001\r\n",
    "epochs = 20\r\n",
    "batch_size = 32\r\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\r\n",
    "criterion = nn.BCELoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "source": [
    "train_csv = pd.read_csv('./Findings.csv')\r\n",
    "# train dataset\r\n",
    "train_data = ImageDataset(\r\n",
    "    train_csv, train=True, test=False\r\n",
    ")\r\n",
    "# validation dataset\r\n",
    "valid_data = ImageDataset(\r\n",
    "    train_csv, train=False, test=False\r\n",
    ")\r\n",
    "# train data loader\r\n",
    "train_loader = DataLoader(\r\n",
    "    train_data, \r\n",
    "    batch_size=batch_size,\r\n",
    "    shuffle=True\r\n",
    ")\r\n",
    "# validation data loader\r\n",
    "valid_loader = DataLoader(\r\n",
    "    valid_data, \r\n",
    "    batch_size=batch_size,\r\n",
    "    shuffle=False\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of training images: 4249\n",
      "Number of validation images: 750\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "print(batch_size)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "# start the training and validation\r\n",
    "train_loss = []\r\n",
    "valid_loss = []\r\n",
    "for epoch in range(epochs):\r\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\r\n",
    "    train_epoch_loss = train(\r\n",
    "        model, train_loader, optimizer, criterion, train_data, device\r\n",
    "    )\r\n",
    "    valid_epoch_loss = validate(\r\n",
    "        model, valid_loader, criterion, valid_data, device\r\n",
    "    )\r\n",
    "    train_loss.append(train_epoch_loss)\r\n",
    "    valid_loss.append(valid_epoch_loss)\r\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\r\n",
    "    print(f'Val Loss: {valid_epoch_loss:.4f}')\r\n",
    "    torch.save({\r\n",
    "            'epoch': epochs,\r\n",
    "            'model_state_dict': model.state_dict(),\r\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\r\n",
    "            'loss': criterion,\r\n",
    "            }, './outputs/model.pth')\r\n",
    "# plot and save the train and validation line graphs\r\n",
    "plt.figure(figsize=(10, 7))\r\n",
    "plt.plot(train_loss, color='orange', label='train loss')\r\n",
    "plt.plot(valid_loss, color='red', label='validataion loss')\r\n",
    "plt.xlabel('Epochs')\r\n",
    "plt.ylabel('Loss')\r\n",
    "plt.legend()\r\n",
    "plt.savefig('./outputs/loss.png')\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 of 20\n",
      "Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/132 [00:00<?, ?it/s]C:\\Users\\anjan\\.conda\\envs\\myenv\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "133it [52:25, 23.65s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validating\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "24it [07:42, 19.27s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Loss: 0.2498\n",
      "Val Loss: 0.2154\n",
      "Epoch 2 of 20\n",
      "Training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/132 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-9306c7fd2d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch+1} of {epochs}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     train_epoch_loss = train(\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     )\n\u001b[0;32m      9\u001b[0m     valid_epoch_loss = validate(\n",
      "\u001b[1;32m<ipython-input-127-8626bcd84bcb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion, train_data, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# apply sigmoid activation to get all the outputs between 0 and 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 338\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('myenv': conda)"
  },
  "interpreter": {
   "hash": "73c07113e4309d48a0d83fa9274956d1de5d545e74cd7d16a25679b47ecc90c7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}